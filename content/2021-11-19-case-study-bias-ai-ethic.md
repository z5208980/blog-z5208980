---
title: "Case Study - Bias AI Ethics"
date: "2021-11-19"
tags: ["Ethics", "AI", "Machine Learning"]
image: ""
gradients: ["#6B73FF", "#000DFF"]
---

## Google AI Service Bias

### 1. Background with IT issue
Google is a technology company that has multiple popular services such as Google Translate, Photos, Images and Search. Google is also one of the many leading companies in commercial Artificial Intelligence (AI). This means that advanced algorithms and training in AI models are designed to help users and clients with problems much more efficiently. But providing a quick and efficient service, Google’s AI algorithms are flawed when it comes to human ethics. On multiple occasions, Google's services such as Translate and Images have suggested biased topics and racist service to their users and clients, which pose the ethical dilemma of AI. These events seem to misalign with Google’s high quality services, as the IT issues are related to the AI models the algorithms have been trained to produce bias against poor, disadvantage or colored individuals based on location and individuality.

### 2. Evidence
#### 2.1 Google Translate Gender Bias
According to an article, “Google Translate systematically changes the gender of translations when they do not fit with stereotypes” [1]. Phrases such as Male nurse and female historian do not exist on the service and researchers found out that the AI algorithm was more masclinced oriented.

#### 2.2 Google’s AI Principle
Google has published their values and principles when considering the use of AIs. They believe that AI should, “1. be socially beneficial” and “2. avoid creating or reinforcing unfair bias” [2]. That includes characteristics (not limited to) such as race, ethnicity and gender.

#### 2.3 Google Images/Photo racist suggestions
Google Images has been reported to have mistakenly tagged dark skin individuals as “gorillas” [3]. Having an recognition algorithm fails to label correctly and create prejudice and racist towards certain groups. The issue

#### 2.4 Google AI employee fired over ethic research
Timnit Gebru, a Google ethic employee was fired due to her research on addressing the inherent bias of large language models. The ethical issue seems that there is no escape from AI algorithms. The models will create inequality judgements, having “the ability to amplify biases against women and minorities” [4].

### 3. Ethical Reasoning
#### 3.1 Google
Google’s ethical reasoning is based on utilitarianism as the purpose of AI is to be in the interest of the users and benefit society. Based on that ethical theory, Google is justified in using AI to provide such services, and it isn't Google that should be held accountable. Rather, the general problem of poor AI algorithms design with unbalanced training datasets. This however means that if taking a deontological approach, then Google must have the duty and responsibility of aligning their AI with human ethics, which they stated they have in section 2.2. This may sound ethical in Kantian ethics, but based on categorical imperative, Google’s creation of biased AI and algorithms, are not universiable as it has produced racism and unfair judgement to different ethnicities (which is overall bad).

#### 3.2 Users
Based on a utilitarian approach, the use of Google’s AI would provide a faster and free service, with the ability for users to progress as a society and create positive change. Current AI decision making is ethical, because it seeks to benefit most of society, even when there may be flaws. On the other hand, users and clients using Google services can encourage more racist and biased data. Just like the dataset, the collection of data from users are more real time and are mostly likely unfiltered with the user’s bias and different characteristics when used for training to help improve quality of AI and user experience. This can influence the overall algorithm affecting and creating more moral issues.

### 4. Ethical arguments
Based section 2.2, Google’s code of ethics in relation to the AIs states that AI should avoid creating or reinforcing unfair bias. Section 2.1 seems to provide an example of Google disabling the function and searching for the problem. In the bigger picture, Google has taken an unethical approach to ‘solving’ it. Removing one case of the problem, can ultimately lead to others cases being overlooked. Overall Google has taken a business approach to justify the use of prejudice AIs and has prevented one case of the issues but not solved them.

Another AI principle from Google is to be socially beneficial. That is AI also enhances our ability to understand the meaning of content at scale. But for certain stakeholders (developers) this isn’t the case. Section 2.4, Google AI ethicist (Timnit Gebru) was fired due to her research on addressing the inherent bias of large language models. It seems that there is no escape that AI algorithms and models will create inequality judgements. Google neglecting in trying to address the AI ethics dilemma and also not acting on solving and answering (by firing 2 AI ethic employees) in Kantian ethics is unethical because Google failed to act on a moral issue and its principles of creating socially beneficial AI.

Even though Google's AI are acting accordingly and providing good outcomes for users, Google shouldn’t be justified based on Utilitarian. This is because Google's facial recognition (Photo) is still being trained with data, which is still less accurate at identifying women and people of color, which means its use can end up discriminating against them. Furthermore, it is important that Google considers the general picture, which is on the ethics of biased AI.

The AI shouldn’t be left as an agent to determine the consequences of a result in a search or translation as it can’t make morally good judgements. From a consequential theory, AI can’t decide if good consequences provide results and can reject the underpinnings of good. An example is the end of humanity to end suffering and pain can be deemed as a good consequence in biased AI.

With an action based ethical approach, biased AI should have a clear understanding of equality and achieving fairness. It may be hard to apply rational thinking to AI and algorithms but with the concept of the AI making decisions to act on something based on the moral rules and characteristics of the user, maybe then it could provide a more fair and suitable service, rather than basing it on what is deemed good.

If we take a virtue ethical approach, biased AI wouldn’t understand the notion of good values and moral character. This ethical argument can be closely aligned to a consequentialist perspective, with AI failing to comprehend the notion of good.

### 5 Conclusion
The fact is that Google’s AI models are based on a massive volume of data generated by humanity from the past. Humans have inherent racial bias, so it only makes sense that the training of AI is influenced by such data we produce and the bias and judgements of the creators. We have to accept the fact that these AIs are machine translation of humans and just like any software or machine, it will never be perfect. But that means that it is more important for Google to reconsider their AI principles in relation to inequality because they are choosing to ignore it.

So to conclude the ethical considerations, Google and biased AI should be based on Kantian ethical theory. Biased AI poses ethical problems in trying to maximise good and determining moral values and virtues. Google should be taken from a Kantian perspective, believing that all people should be treated equally and respectfully and AI should take Kantian perspective because rather than pondering on the good and happiness as parameters, it is up to the actions of the AI that determine morality and justification to the user.

#### 5.1 Possible Steps
Google can redress the issue ethically by first understanding that they need to improve the training dataset. It’s clear that even if they are improving the data, they are not considering a diverse group of people and balancing out the data. Google could use analytical techniques and add params related to social equalities so that the AI’s are taking into consideration the issues and hence reducing scenarios of bias and unfairness in their services.

Another step is to use more human centric approaches when testing and training AI with datasets that have data statements. This can be effective as Google can have a better understanding of what the data contains and what data is being used to train their AIs. Finally, a more traditional approach is to test the AI and algorithms in different environments. With this idea, it can allow better exposure for AI to generate outcomes that are more representative of the wider population.

### References
1.  Nicolas Kayser-Bri. 2020. Female historians and male nurses do not exist, Google Translate tells its European users - AlgorithmWatch. [online] Available at: <https://algorithmwatch.org/en/google-translate-gender-bias/> [Accessed 05 November 2021].
    
2.  Google. 2018. Artificial Intelligence at Google: Our Principles - Google. [online] Available at: <[https://ai.google/principles/](https://ai.google/principles/)> [Accessed 05 November 2021].
    
3.  James Vincent. 2018. Google ‘fixed’ its racist algorithm by removing gorillas from its image-labeling tech - The Verge. [online] Available at: <[https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai](https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai) > [Accessed 08 November 2021].
    
4.  Shirin Ghaffary. 2021. Google says it’s committed to ethical AI research. Its ethical AI team isn’t so sure. - Vox. [online] Available at: <[https://www.vox.com/recode/22465301/google-ethical-ai-timnit-gebru-research-alex-hanna-jeff-dean-marian-croak](https://www.vox.com/recode/22465301/google-ethical-ai-timnit-gebru-research-alex-hanna-jeff-dean-marian-croak)> [Accessed 08 November 2021].